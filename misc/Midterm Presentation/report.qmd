---
format: pdf
author: Michael Throolin
title: My fancy report on FIDDLE
subtitle: For PHS 7045
bibliography: references.bib
---

# Introduction

Electronic health record (EHR) data and machine learning tools have been useful in the realm of developing patient risk stratification models. However, EHR is often suffers from the curse of dimensionality and has irregular sampling. As such, before a researcher can use machine learning tools they have to preprocess the data, which is often tedious and time consuming. The preprocessing often requires decisions regarding variable selection, data resampling, and dealing with missing values. Such decisions vary across studies. FIDDLE [@FIDDLE] was proposed as an algorithm to streamline this process.


FIDDLE (Flexible Data-Driven Pipeline) is a systematic approach to transform structured EHR data into ML-ready representations. It is a data-driven approach which allows for some customization. The preprocessing steps of FIDDLE, as described by [@FIDDLE], involve three main stages:

**Pre-filter:**

- Remove rows with timestamps outside the observation period and eliminate rarely occurring variables to speed up downstream analysis.

**Transform:**

- Time-invariant data are concatenated into a table.

- Time-dependent data are concatenated into a tensor, with non-frequent variables represented by their most recent recorded value and frequent variables represented by summary statistics.

- Missing values are handled using carry-forward imputation with a "presence mask" to track imputed values.

**Post-filter:**

- Features that are unlikely to be informative, such as those that are almost constant across examples, are removed. Duplicated features are also combined into a single feature.

These preprocessing steps are designed to systematically transform structured EHR data into feature vectors that can be used for machine learning models.

IDDLE code was developed in Python ([github repo](https://github.com/MLD3/FIDDLE)). For my project I am translating this code to R. This is part of a group assignment I am doing for my EHR class where we will propose improvements to FIDDLE. I will code these improvements in R.




# Description of the solution plan

From my preliminary reading of the python code, I have identified the following steps to translate the code to R:

1. Change items stored as a data frame to a data table
2. Use Rcpp to speed up the code where possible.
3. Parallelize what I can. As this is data preprocessing I can probably split the data up  to do this once I have all the functions up and running.

After I have finished the main translation, I plan to explore a couple changes to the FIDDLE pipeline:

- Use a different imputation method for missing values. The last value carried forward may not be feasible in every pipeline.

- Use a different method for removing features that are unlikely to be informative. The method used in the FIDDLE pipeline is based on constant thresholds rather than an score-based system. I could implement some feature importance metric for this filtering.


# Preliminary results

For the sake of this midterm, I have preprogrammed the pre-filter step in R and C++. I simulated some data to verify both codes produce the same results, then used microbenchmark to run the Rcpp code 100 times and to determine which was faster, my data.table code or my Rcpp code. You can check my code inside the [code folder of this project's github repo](https://github.com/mthroolin/phs7045midterm/tree/main/code).


A summary of the simulated data:

- 995165 rows of data

- 10000 subject-specific IDs

- 100 variables, 90 numerical and 10 categorical

- Time measurements between 0 and 10


Below I show how the data looks after it is processed as well as the timing results of the pre-filter code in R versus C++. It appears that using data.table in R was much faster than using Rcpp. 



A sample of how the data looks after the pre-filter step:
```{r setup}
#| label: some-code
#| echo: false

# Load libraries
library(microbenchmark); library(data.table); library(Rcpp)
set.seed(10162024)
source("code/generate_data.R")
source("code/pre_filter.R")
sourceCpp("code/pre_filter.cpp")


result_R <- pre_filter_dt(df, df_population, threshold, max.T, var_type_override)
result_Cpp <- pre_filter_cpp(df, df_population, threshold, max.T, var_type_override)

head(result_R)
```


Timing Results:

```{r timing}
#| echo: false
#| cache: true
# Benchmarking
set.seed(10162024)
benchmark_result <- microbenchmark(
  R_version = {
    result_R <- pre_filter_dt(df, df_population, threshold, max.T, var_type_override)
  },
  Cpp_version = {
    result_Cpp <- pre_filter_cpp(df, df_population, threshold, max.T, var_type_override)
  },
  times = 5  # Number of times to run each function
)

# Display benchmark results
print(benchmark_result)
```




# Appendix

## References